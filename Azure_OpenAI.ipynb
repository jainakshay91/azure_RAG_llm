{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jainakshay91/LLM_Docker/blob/ftr%2Fazure_openai/Azure_OpenAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sV_fw0LGG0Qn",
    "outputId": "9d947684-1773-446a-9d13-5b027124703d"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "# Setup Azure Open AI \n",
    "\n",
    "\n",
    "with open(\"./key.txt\", 'r') as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        value = line.split('=')[1].strip()\n",
    "\n",
    "        if line.startswith(\"openai.api_base\"):\n",
    "            os.environ[\"OPENAI_API_BASE\"] = value\n",
    "\n",
    "        elif line.startswith(\"API key\"):\n",
    "            os.environ[\"OPENAI_API_KEY\"] = value\n",
    "            \n",
    "        elif line.startswith(\"deployment_name_text\"):  \n",
    "            deployment_text = value\n",
    "\n",
    "        elif line.startswith(\"deployment_name_embed\"):\n",
    "            deployment_embed = value\n",
    "\n",
    "client = AzureOpenAI(\n",
    "  azure_endpoint = os.getenv(\"OPENAI_API_BASE\"),\n",
    "  api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "  api_version=\"2024-02-15-preview\"\n",
    ")\n",
    "\n",
    "#################################################################\n",
    "# Data Path: Dataset and Embeddings Database\n",
    "#################################################################\n",
    "\n",
    "database_path = \"./Dataset/\"\n",
    "embed_path = \"./Embed/\"\n",
    "completion_model_name = deployment_text\n",
    "embedding_model_name = deployment_embed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rO6SIzbKG3nO",
    "outputId": "dc6891fd-5d16-470f-8287-22d075724244"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(757, 2)\n"
     ]
    }
   ],
   "source": [
    "def LLMsetter(message_text):\n",
    "  completion = client.chat.completions.create(\n",
    "    model=completion_model_name, # model = \"deployment_name\"\n",
    "    messages = message_text,\n",
    "    temperature=0.2,\n",
    "    max_tokens=800,\n",
    "    top_p=0.95,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    "    stop=None,\n",
    "  )\n",
    "\n",
    "  return completion\n",
    "\n",
    "def EmbeddingGenerator(text):\n",
    "  return client.embeddings.create(model=embedding_model_name, input = text, encoding_format='float').data[0].embedding\n",
    "\n",
    "def TextSplittersetter():\n",
    "  # Setup the Text Splitter\n",
    "  text_splitter = RecursiveCharacterTextSplitter(separators=[\n",
    "          \"\\n\\n\",\n",
    "          \"\\n\",\n",
    "          \" \",\n",
    "          \".\",\n",
    "          \",\",\n",
    "          \"\\u200B\",  # Zero-width space\n",
    "          \"\\uff0c\",  # Fullwidth comma\n",
    "          \"\\u3001\",  # Ideographic comma\n",
    "          \"\\uff0e\",  # Fullwidth full stop\n",
    "          \"\\u3002\",  # Ideographic full stop\n",
    "          \"\",\n",
    "      ],\n",
    "      chunk_size=5000,\n",
    "      chunk_overlap=200,\n",
    "      length_function=len,\n",
    "      is_separator_regex=False,\n",
    "  )\n",
    "  return text_splitter\n",
    "\n",
    "def data_vectorstore_generator(text_splitter):\n",
    "  # Create loaders for the multiple files inside the data path\n",
    "\n",
    "  loaders = []\n",
    "  for f in os.listdir(database_path):\n",
    "    if \"ipynb\" in f:\n",
    "      continue\n",
    "    else:\n",
    "      loaders.append(PyPDFLoader(os.path.join(database_path,f)))\n",
    "\n",
    "  chunked_documents = []\n",
    "  # Initialize the database\n",
    "\n",
    "  db = pd.DataFrame(columns=[\"Text\"])\n",
    "\n",
    "  # Chunking multiple PDFs in the database path\n",
    "  for loader in loaders:\n",
    "    # Load the Files to be chunked\n",
    "    docs = loader.load()\n",
    "    # Chunk the loaded PDF\n",
    "    texts = text_splitter.split_documents(documents = docs)\n",
    "    #print(texts[0].page_content)\n",
    "    for text in texts:\n",
    "      db.loc[len(db.index)] = text.page_content\n",
    "  #print(db)\n",
    "  db[\"Embeddings\"] = db[\"Text\"].apply(lambda x: EmbeddingGenerator(x))\n",
    "  #print(db)\n",
    "\n",
    "  return db\n",
    "\n",
    "def cosine_similarity_compute(query_embed,data_embed):\n",
    "  return np.dot(query_embed, data_embed) / (np.linalg.norm(query_embed) * np.linalg.norm(data_embed))\n",
    "\n",
    "def context_generator(query, db, top_n):\n",
    "\n",
    "  # Generate Query Embedding\n",
    "  query_embedding = client.embeddings.create(model=embedding_model_name,input=query).data[0].embedding\n",
    "\n",
    "  # Compute the Cosine Similarity Index with all the Embeddings\n",
    "  db[\"Similarities\"] = db.Embeddings.apply(lambda x: cosine_similarity_compute(x,query_embedding))\n",
    "\n",
    "  # Sort the Similarity values and compute the top N closest similar texts for the context\n",
    "  search_result = db.sort_values(\"Similarities\",ascending=False).head(top_n)\n",
    "\n",
    "  # Join the resultant search result to create the context\n",
    "  context = ' '.join(search_result.Text)\n",
    "  #print(context)\n",
    "\n",
    "  return context\n",
    "\n",
    "def main():\n",
    "\n",
    "  # Define the Size of Context\n",
    "  top_n = 4\n",
    "\n",
    "  # Initialize the Text Splitter\n",
    "  text_splitter = TextSplittersetter()\n",
    "\n",
    "  # Generate the Data Vector Store\n",
    "  # If the Vector Store was previously generated then pull the pickled database to avoid computing embeddings again\n",
    "  if os.path.isfile(embed_path+\"db.pkl\"):\n",
    "    db = pd.read_pickle(embed_path+\"db.pkl\")\n",
    "  else:\n",
    "    db = data_vectorstore_generator(text_splitter)\n",
    "    db.to_pickle(embed_path+\"db.pkl\")\n",
    "  print(db.shape)\n",
    "  #print(db.Embeddings)\n",
    "  #print(db.Embeddings.shape)\n",
    "  query = input(\"Insert your Query \\n\")\n",
    "\n",
    "  context = context_generator(query, db, top_n)\n",
    "\n",
    "\n",
    "  #System Role and Content Setup\n",
    "  system_setup = [{\"role\":\"system\",\"content\":\"You are an AI assistant that helps people find information. Your responses are human like and yet technically correct as much as possible\"},\n",
    "                  {\"role\":\"user\",\"content\":query},\n",
    "                 {\"role\":\"assistant\", \"content\":f\"Relevat Information for generating output: \\n {context}\"}]\n",
    "\n",
    "  response = LLMsetter(system_setup)\n",
    "\n",
    "  # Print response\n",
    "\n",
    "  print(\"Response: \" + response.choices[0].message.content + \"\\n\")\n",
    "    \n",
    "\n",
    "# Calling the main function\n",
    "\n",
    "\n",
    "main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMX2GH3S5yYE+Q5yImqMvoy",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
